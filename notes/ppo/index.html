<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Trust Region and Proximal Methods | Mike Zhou </title> <meta name="author" content="Mike Zhou"> <meta name="description" content=""> <meta name="keywords" content="math-notes"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mzhou02.github.io/notes/ppo/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Mike</span> Zhou </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/notes/">notes <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">about </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Trust Region and Proximal Methods</h1> <p class="post-description"></p> </header> <article> <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-0823RLC0T3"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0823RLC0T3");</script> <h2>Trust Region Methods in Reinforcement Learning</h2> <h4>Beyond Value-Based Methods: The Policy Gradient Approach</h4> <p>While value-based methods like Q-learning provide powerful tools for reinforcement learning, they operate indirectly by first learning a value function and then deriving a policy. An alternative approach is to parameterize the policy directly and optimize it to maximize expected returns. This policy gradient approach offers several advantages, particularly for problems with continuous action spaces or where stochastic policies are beneficial.</p> <p>In the policy gradient framework, we represent the policy as a parameterized function \(\pi_\theta(a\mids)\), where \(\theta\) are the parameters (e.g., weights of a neural network). The objective is to find parameters \(\theta\) that maximize the expected return:</p> \[J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \gamma^t r_t \right]\] <p>where \(\tau\) represents a trajectory (sequence of states, actions, and rewards) sampled by following policy \(\pi_\theta\).</p> <p>The policy gradient theorem provides a way to compute the gradient of this objective:</p> \[\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t\mids_t) \cdot G_t \right]\] <p>where \(G_t = \sum_{k=t}^{T} \gamma^{k-t} r_k\) is the return from time step \(t\).</p> <p>This formula leads to a straightforward algorithm: sample trajectories using the current policy, compute the gradients according to the above formula, and update the parameters using gradient ascent:</p> \[\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)\] <p>While this approach is theoretically sound, it often suffers from high variance in gradient estimates, poor sample efficiency, and sensitivity to step size. These limitations have motivated the development of more sophisticated policy optimization methods, including trust region approaches.</p> <h4>The Stability Challenge in Policy Optimization</h4> <p>Standard policy gradient methods face a fundamental challenge: determining the appropriate step size for parameter updates. Too small a step and learning progresses slowly; too large a step and the policy may change drastically, potentially degrading performance and leading to unstable learning.</p> <p>This challenge becomes particularly acute when using function approximation, such as neural networks, to represent the policy. In such cases, a seemingly small change in parameters might result in a dramatic change in behavior. Moreover, the reinforcement learning objective is non-stationary and typically non-convex, making optimization inherently difficult.</p> <p>The issue can be understood through the lens of catastrophic forgetting. Consider fine-tuning a language model on a specific task: aggressive optimization toward task performance might cause the model to rapidly diverge from its pre-trained state, losing the general language understanding that made it valuable in the first place. The model excels at the narrow task but forgets its broader capabilities.</p> <p>Trust region methods address this challenge by imposing constraints on how much the policy can change in a single update, ensuring that learning remains stable while still making meaningful progress.</p> <h2>Trust Region Policy Optimization (TRPO)</h2> <h4>The Theoretical Foundation: Conservative Policy Iteration</h4> <p>Trust Region Policy Optimization (TRPO), introduced by Schulman et al. in 2015, builds upon theoretical work in conservative policy iteration. The key insight is that policy updates should be constrained to prevent large, potentially harmful changes.</p> <p>Consider the relationship between policies and their performance. If we have a current policy \(\pi_{\text{old}}\) and are considering a new policy \(\pi\), we can express the expected return of \(\pi\) in terms of the advantage function of \(\pi_{\text{old}}\):</p> \[J(\pi) = J(\pi_{\text{old}}) + \mathbb{E}_{s \sim \rho_\pi, a \sim \pi} \left[ A^{\pi_{\text{old}}}(s, a) \right]\] <p>where \(\rho_\pi\) is the state visitation frequency under policy \(\pi\), and \(A^{\pi_{\text{old}}}(s, a)\) is the advantage function, which measures how much better it is to take action \(a\) in state \(s\) compared to the average action according to \(\pi_{\text{old}}\).</p> <p>This equation is exact, but it’s problematic for optimization because the state distribution \(\rho_\pi\) depends on the new policy \(\pi\), which we’re trying to find. To address this, TRPO relies on a local approximation:</p> \[J(\pi) \approx J(\pi_{\text{old}}) + \mathbb{E}_{s \sim \rho_{\pi_{\text{old}}}, a \sim \pi} \left[ A^{\pi_{\text{old}}}(s, a) \right]\] <p>This approximation assumes that the state visitation frequencies under the old and new policies are similar, which is reasonable if the policies themselves are similar. But how do we ensure that the policies remain similar enough for this approximation to hold?</p> <h4>Constraining Policy Updates with KL Divergence</h4> <p>TRPO formalizes the notion of “similar policies” using the Kullback-Leibler (KL) divergence, a measure of how one probability distribution differs from another. Specifically, TRPO constrains the maximum KL divergence between the old and new policies at any state:</p> \[\max_s D_{KL}(\pi_{\text{old}}(\cdot\mids) \parallel \pi(\cdot\mids)) \leq \delta\] <p>where \(\delta\) is a hyperparameter controlling the size of the trust region.</p> <p>The TRPO optimization problem thus becomes:</p> <p>\(\max_{\theta} \mathbb{E}_{s \sim \rho_{\pi_{\text{old}}}, a \sim \pi_\theta} \left[ A^{\pi_{\text{old}}}(s, a) \right]\) \(\text{subject to } \max_s D_{KL}(\pi_{\text{old}}(\cdot\mids) \parallel \pi_\theta(\cdot\mids)) \leq \delta\)</p> <p>This constrained optimization ensures that the new policy doesn’t deviate too far from the old one, maintaining the validity of the local approximation and preventing catastrophic updates.</p> <h4>The TRPO Algorithm</h4> <p>Solving the constrained optimization problem directly is challenging. TRPO uses a series of approximations:</p> <ol> <li> <p>Replace the maximum KL divergence with the average KL divergence: \(\mathbb{E}_{s \sim \rho_{\pi_{\text{old}}}} \left[ D_{KL}(\pi_{\text{old}}(\cdot\mids) \parallel \pi_\theta(\cdot\mids)) \right] \leq \delta\)</p> </li> <li> <p>Use a linear approximation for the objective and a quadratic approximation for the constraint: \(\max_{\theta} g^T (\theta - \theta_{\text{old}})\) \(\text{subject to } \frac{1}{2} (\theta - \theta_{\text{old}})^T H (\theta - \theta_{\text{old}}) \leq \delta\)</p> <p>where \(g\) is the gradient of the objective and \(H\) is the Hessian of the KL divergence with respect to \(\theta\).</p> </li> <li> <p>Solve this approximated problem using the natural gradient: \(\theta_{\text{new}} = \theta_{\text{old}} + \sqrt{\frac{2\delta}{g^T H^{-1} g}} H^{-1} g\)</p> </li> </ol> <p>In practice, computing the Hessian \(H\) and its inverse is computationally expensive for large neural networks. TRPO approximates the Hessian-vector products using the conjugate gradient algorithm and uses backtracking line search to ensure that the KL constraint is satisfied while maximizing the objective.</p> <p>The complete TRPO algorithm involves:</p> <ol> <li>Collecting trajectories using the current policy</li> <li>Estimating advantages using, for example, Generalized Advantage Estimation (GAE)</li> <li>Computing the search direction using conjugate gradient</li> <li>Performing a line search to find the optimal step size</li> <li>Updating the policy parameters</li> </ol> <h4>Strengths and Limitations of TRPO</h4> <p>TRPO has several notable strengths:</p> <ul> <li>It provides monotonic policy improvement guarantees under certain conditions</li> <li>It’s relatively robust to hyperparameter choices, particularly the step size</li> <li>It has demonstrated strong performance across a variety of continuous control tasks</li> </ul> <p>However, TRPO also has significant limitations:</p> <ul> <li>It’s computationally expensive due to the second-order optimization</li> <li>The constrained optimization problem is complex to implement</li> <li>It doesn’t naturally extend to architectures with shared parameters between policy and value functions</li> <li>The exact constraint enforcement can sometimes be overly conservative</li> </ul> <p>These limitations motivated the development of simpler alternatives that retain the key insights of trust region methods while being more practical to implement.</p> <h2>Proximal Policy Optimization (PPO)</h2> <h4>Simplifying Trust Region Methods</h4> <p>Proximal Policy Optimization (PPO), introduced by Schulman et al. in 2017, aims to capture the benefits of TRPO while addressing its practical limitations. PPO maintains the core idea of restricting policy updates to a trust region but employs a simpler approach that avoids second-order optimization.</p> <p>The key insight of PPO is that we can achieve similar performance to TRPO using a clipped surrogate objective function that can be optimized with first-order methods like stochastic gradient descent.</p> <h4>The Clipped Surrogate Objective</h4> <p>PPO works with the ratio of probabilities between the new and old policies:</p> \[r_t(\theta) = \frac{\pi_\theta(a_t\mids_t)}{\pi_{\theta_{\text{old}}}(a_t\mids_t)}\] <p>This ratio indicates how much more (or less) likely the action \(a_t\) is under the new policy compared to the old policy. If \(r_t(\theta) &gt; 1\), the new policy assigns higher probability to the action; if \(r_t(\theta) &lt; 1\), it assigns lower probability.</p> <p>The standard surrogate objective would be:</p> \[L^{SURR}(\theta) = \mathbb{E}_t \left[ r_t(\theta) \cdot A_t \right]\] <p>where \(A_t\) is the estimated advantage at time \(t\).</p> <p>Maximizing this objective would increase the probability of actions with positive advantages and decrease the probability of actions with negative advantages. However, without constraints, this could lead to excessively large policy updates.</p> <p>PPO addresses this by clipping the probability ratio:</p> \[L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min(r_t(\theta) \cdot A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \cdot A_t) \right]\] <p>where \(\epsilon\) is a hyperparameter (typically around 0.2) that controls the clipping range.</p> <p>This clipped objective has an intuitive interpretation:</p> <ul> <li>For actions with positive advantages (\(A_t &gt; 0\)), the objective encourages increasing their probability, but only up to a point (\(r_t(\theta) \leq 1+\epsilon\)).</li> <li>For actions with negative advantages (\(A_t &lt; 0\)), the objective encourages decreasing their probability, but again, only up to a point (\(r_t(\theta) \geq 1-\epsilon\)).</li> </ul> <p>The clipping mechanism effectively creates a trust region that prevents the new policy from deviating too far from the old one.</p> <h4>The Complete PPO Algorithm</h4> <p>The complete PPO algorithm typically includes additional components:</p> <ol> <li> <p><strong>Value function estimation</strong>: PPO often optimizes a value function \(V_\phi(s)\) alongside the policy, using a loss function like: \(L^{VF}(\phi) = \mathbb{E}_t \left[ (V_\phi(s_t) - V_t^{target})^2 \right]\)</p> </li> <li> <p><strong>Entropy bonus</strong>: To encourage exploration, PPO may include an entropy term in the objective: \(L^{ENT}(\theta) = \mathbb{E}_t \left[ H(\pi_\theta(\cdot\mids_t)) \right]\)</p> </li> <li> <p><strong>Combined objective</strong>: The final objective combines these components: \(L^{TOTAL}(\theta, \phi) = \mathbb{E}_t \left[ L^{CLIP}(\theta) - c_1 L^{VF}(\phi) + c_2 L^{ENT}(\theta) \right]\) where \(c_1\) and \(c_2\) are hyperparameters controlling the relative importance of each term.</p> </li> </ol> <p>The PPO algorithm proceeds as follows:</p> <pre>
Algorithm: Proximal Policy Optimization (PPO)
1. Initialize policy parameters θ and value function parameters φ
2. Repeat:
   a. Collect set of trajectories by running policy π_θ in the environment
   b. Compute advantages using generalized advantage estimation or another method
   c. Compute rewards-to-go for value function targets
   d. For each epoch:
      i. For each minibatch:
         1. Update θ by maximizing the PPO-Clip objective
         2. Update φ by minimizing the value function loss
3. Until convergence
</pre> <h4>PPO Variants and Implementation Details</h4> <p>There are several common variants and implementation details that can significantly affect PPO’s performance:</p> <ol> <li> <p><strong>PPO-Clip vs. PPO-Penalty</strong>: Besides the clipping approach described above, an alternative is to use a KL penalty term, similar to TRPO but without explicit constraint enforcement: \(L^{PENALTY}(\theta) = \mathbb{E}_t \left[ r_t(\theta) \cdot A_t - \beta \cdot D_{KL}(\pi_{\theta_{\text{old}}}(\cdot\mids_t) \parallel \pi_\theta(\cdot\mids_t)) \right]\) where \(\beta\) is adapted based on whether the actual KL divergence is above or below a target value.</p> </li> <li> <p><strong>Normalized advantages</strong>: To reduce variance, advantages are often normalized to have zero mean and unit variance within each batch.</p> </li> <li> <p><strong>Value function clipping</strong>: Some implementations also clip the value function updates to prevent large changes: \(L^{VF-CLIP}(\phi) = \mathbb{E}_t \left[ \max((V_\phi(s_t) - V_t^{target})^2, (V_{\phi_{\text{old}}}(s_t) + \text{clip}(V_\phi(s_t) - V_{\phi_{\text{old}}}(s_t), -\epsilon, \epsilon) - V_t^{target})^2) \right]\)</p> </li> <li> <p><strong>Minibatch updates</strong>: Rather than updating on all collected data at once, PPO typically uses minibatch updates.</p> </li> <li> <p><strong>Orthogonal initialization and layer normalization</strong>: These techniques can improve training stability and performance.</p> </li> </ol> <p>In practice, PPO has become more widely adopted due to its simplicity and strong empirical performance, while TRPO remains valuable in contexts where theoretical guarantees are paramount or where the additional computational cost is justified.</p> <h2>Extensions and Applications of Trust Region Methods</h2> <h4>Generalized Advantage Estimation (GAE)</h4> <p>A critical component of both TRPO and PPO is advantage estimation. Generalized Advantage Estimation (GAE), also introduced by Schulman et al., provides a way to trade off bias and variance in advantage estimates:</p> \[A^{GAE(\gamma, \lambda)}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}\] <p>where \(\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)\) is the TD error at time \(t\), and \(\lambda \in [0, 1]\) is a hyperparameter controlling the bias-variance trade-off.</p> <p>GAE with \(\lambda = 0\) corresponds to using TD errors as advantages (low variance but high bias), while \(\lambda = 1\) corresponds to using Monte Carlo returns minus the baseline (high variance but low bias). Intermediate values of \(\lambda\) interpolate between these extremes.</p> <h4>Trust Region Methods for Fine-Tuning Large Models</h4> <p>Trust region methods have found important applications in fine-tuning large pre-trained models, such as language models, for specific tasks. The challenge in such contexts is to adapt the model to perform well on the target task without forgetting the general knowledge encoded in its parameters.</p> <p>For example, when fine-tuning a language model on a specific text generation task, a KL divergence penalty between the original and fine-tuned model can prevent catastrophic forgetting:</p> \[L(\theta) = L_{task}(\theta) - \beta \cdot D_{KL}(\pi_{\theta_0}(\cdot\mids) \parallel \pi_\theta(\cdot\mids))\] <p>where \(L_{task}(\theta)\) is the task-specific loss, \(\pi_{\theta_0}\) is the original model, and \(\beta\) controls the strength of the regularization.</p> <p>This approach ensures that the fine-tuned model doesn’t deviate too far from the original, preserving its general capabilities while adapting to the specific task.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Mike Zhou. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>