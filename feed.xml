<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://mikezhou.me/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mikezhou.me/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-15T15:33:19+00:00</updated><id>https://mikezhou.me/feed.xml</id><title type="html">blank</title><subtitle>Mike Zhou - Computer Science and Mathematics student at the University of Pennsylvania (UPenn). Undergraduate researcher at Penn&apos;s Cognitive Computation Group working on NLP and machine learning. </subtitle><entry><title type="html">The Gradient of Thought: Reasoning as a Natural Objective for Alignment</title><link href="https://mikezhou.me/blog/2025/gradient-of-thought/" rel="alternate" type="text/html" title="The Gradient of Thought: Reasoning as a Natural Objective for Alignment"/><published>2025-10-22T18:34:00+00:00</published><updated>2025-10-22T18:34:00+00:00</updated><id>https://mikezhou.me/blog/2025/gradient-of-thought</id><content type="html" xml:base="https://mikezhou.me/blog/2025/gradient-of-thought/"><![CDATA[<p>Large language models (LLMs) learn in a remarkably simple way: by predicting the next token in a sequence. During pre-training, they process vast corpora and adjust parameters $\theta$ so that the probability assigned to the correct next token,</p> \[p_\theta(x_t \mid x_{&lt;t}),\] <p>steadily rises. This is the familiar cross-entropy language-modeling objective.</p> <p>At a high level, an LLM learns a map from <em>context</em>, what has already been seen, to <em>expectation</em>, a distribution for what should come next,. Fine-tuning, or post-training, refines this map for specific tasks: multi-step reasoning, following instructions, calibration, and so on.</p> <p>A by-now standard empirical observation is that prompting models to produce Chain-of-Thought (CoT), step-by-step intermediate explanations, often yields striking improvements on reasoning tasks<sup><a href="#ref1">[1]</a></sup>. Appending "Let’s think step by step" induces the model to unfold a short sequence of intermediate tokens that articulate a path:</p> <blockquote style="margin-left: 2.5em; margin-right: 2.5em; font-size: 0.9em; color: #444; font-style: italic;"> "To solve this, first note the number is even; therefore dividing by two gives..." </blockquote> <p>Why should such intermediate text help?</p> <p><br/></p> <h1> An Information-Theoretic Perspective </h1> <p><br/></p> <p>Next-token prediction can be viewed as an exercise in uncertainty management. The <em>surprisal</em> of a token $x_t$ given its context $x_{&lt;t}$ is</p> \[\text{surp}(x_t \mid x_{&lt;t}) \;=\; -\log p_\theta(x_t \mid x_{&lt;t}),\] <p>measured in bits. Surprisal counts how many “yes/no” questions one would need, on average, to identify the correct token under the model’s own distribution. High surprisal signals a difficult prediction (the model did not expect this token); low surprisal signals an easy one.</p> <p>When a question is hard, we rarely vault straight to the answer. We instead lay down stepping stones: small statements that are easy to verify, each making the next step easier. CoT plays the same role for a model. Rather than a single long, unlikely jump $q \to a$, we nudge the model to traverse a short path</p> \[q \;\to\; r_1 \;\to\; r_2 \;\to\; \cdots \;\to\; r_k \;\to\; a,\] <p>so that each conditional $p_\theta(r_i \mid q, r_{&lt;i})$ and $p_\theta(a \mid q, r_{1:k})$ is higher than it would have been without the preceding scaffolding. The informational load that would otherwise concentrate in a single cliff-edge step becomes distributed across the sentence in several gentler slopes.</p> <p><br/></p> <h1> Chain-of-Thought and Post Training Gradients </h1> <p><br/></p> <p>One can think of post-training not as the acquisition of new facts, but as the adjustment of geometry within a model that already knows much. Because language models are optimized under a cross-entropy objective, <strong>surprisal and gradient behavior are intimately linked</strong>: when the model’s predictions are more evenly distributed, its updates become steadier. Fine-tuning on Chain-of-Thought prompt-answer pairs achieves precisely this effect. The reasoning prefix redistributes surprisal across tokens, smoothing the loss surface and dampening abrupt fluctuations in the gradient with respect to the logits, what we may call gradient spikes. In essence, CoT supervision transforms learning from a sequence of sharp corrections into a flow of gentle, coherent updates.</p> <p><br/></p> <div class="card mt-3 p-3"> <h6 class="card-title font-weight-medium">Lemma 1</h6> <div class="card-text"> <p>The partial derivative of cross-etropy loss with respect to logit $x_k$ is simply the model’s predicted probability of that token minus the true label. In other words, $$\frac{\partial \mathcal{L}}{\partial x_k} = p_\theta(y_t \mid y_{&lt;t}) - \mathbb{I}\{x_k = y_t\}, $$ where $y_t$ is the correct next token in the training data (assuming softmax activation). </p> </div> </div> <p><br/></p> <p><em>Proof.</em> Write $Z = \sum_j e^{x_j}$ and $p_i = e^{x_i}/Z$. Let $q$ denote the one-hot distribution from the training sample and $p_k$ represent the probability of predicting token $x_k$ from context $y_{&lt;t}$.</p> <p>For each $i$,</p> \[\log p_i = x_i - \log Z,\] <p>so</p> \[\frac{\partial}{\partial x_k} \log p_i = \frac{\partial x_i}{\partial x_k} - \frac{\partial \log Z}{\partial x_k} = \mathbb{I}\{i = k\} - \frac{1}{Z} \frac{\partial Z}{\partial x_k}\] <p><br/></p> \[= \mathbb{I}\{i = k\} - \frac{e^{x_k}}{Z} = \mathbb{I}\{i = k\} - p_k.\] <p>Therefore,</p> \[\frac{\partial \mathcal{L}}{\partial x_k} = -\sum_{i=1}^V q(x_i) \frac{\partial}{\partial x_k} \log p_i = -\sum_i q(x_i)\bigl(\mathbb{I}\{i = k\} - p_k\bigr)\] <p><br/></p> \[= -q(x_k) + p_k \sum_i q(x_i) = p_k - q(x_k),\] <p>since $\sum_i q(i) = 1$.</p> <p>Specializing to the one-hot case $q = \mathbb{I}\{x_k = y_t\}$ gives</p> \[\frac{\partial \mathcal{L}}{\partial x_k} = p_k - \mathbb{I}\{x_k = y_t\},\] <p>as claimed. $\square$</p> <p><br/> We now use this to connect the size of the gradient to the model’s probability distribution.</p> <p><br/></p> <div class="card mt-3 p-3"> <h6 class="card-title font-weight-medium">Lemma 2</h6> <div class="card-text"> <p>Let $g$ be the loss gradient in respect to model logits. If $\|g\|^2 = \tau$, then $$ 1 - \sqrt{\tau} \leq p_\theta(y_t \mid y_{&lt;t}) \leq 1 - \sqrt{\tfrac{\tau}{2}}. $$ </p> </div> </div> <p><br/></p> <p><em>Proof.</em> Let $p$ denote the probability vector $p_\theta(\cdot \mid y_{&lt;t})$ and $p_y$ represent $p_\theta(y_t \mid y_{&lt;t})$. From Lemma 1, the gradient with respect to the logits satisfies</p> \[\|g\|^2 = (1 - p_y)^2 + \sum_{x_i \neq y_t} p_\theta(x_i \mid y_{&lt;t})^2.\] <p>Rewriting,</p> \[\|g\|^2 = \|p\|_2^2 - 2p_y + 1.\] <p>Since the entries of $p$ are nonnegative and sum to one,</p> \[\|p\|_2^2 \leq p_y^2 + (1 - p_y)^2.\] <p>Substituting this bound gives</p> \[\|g\|^2 \leq 2(1 - p_y)^2.\] <p>Hence, if $|g|^2 = \tau$, it follows that</p> \[p_y \leq 1 - \sqrt{\tfrac{\tau}{2}},\] <p>Similarly, we can use the bound</p> \[|p\|_2^2 \geq p_y^2\] <p>to get that</p> \[p_y \geq 1 - \sqrt{\tau}. \square\] <p><br/> The magnitude of the gradient spike during training is thus largely governed by the tail of the model’s probability distribution: when the model assigns high probability to the correct token, the gradient necessarily remains small. Reasoning trace data is easier for the model to predict, effectively shifting the distribution of the correct token forward, moving the tail upwards and thereby reducing the frequency and severity of large gradients in practice.</p> <p>Of course, if we used only the preceding lemma, one could only guarantee this in theory when the shift in probability exceeds roughly $\sqrt{\tau_{\text{direct}}}(1 - \sqrt{2}/2)$ in factor; the verification of this is left as an exercise to the reader. However, the inequalities employed above are somewhat generous, and in most realistic settings the stability improvement from CoT training probably appears far stronger than this conservative bound would suggest.</p> <p><br/></p> <h1> Empirical Results </h1> <p><br/></p> <p>Despite several attempts, I was not able to extend this proof to the model parameters without running into messy details with attention Jacobians. So, to give some empirical footing, I carried out a small experiment on Llama-3.1-8B-Instruct, using the GSM8K dataset as a simple and interpretable test bed.</p> <p>The model was fine-tuned with full parameter updates using a batch size of 4, a learning rate of $1 \times 10^{-5}$, and a random seed of 42 across all runs. Since most instruction-tuned models already display some latent reasoning behavior when prompted, the goal was not to introduce reasoning, but to vary the degree to which it is supervised.</p> <p>Instead of training solely on final answers, two supervision regimes were used:</p> <ul> <li><strong>Direct:</strong> half of the reasoning steps in each explanation were randomly removed from the sample, leaving the remaining steps (and the final answer) supervised.</li> <li><strong>Full-CoT:</strong> all reasoning and answer tokens were included in the training objective.</li> </ul> <p>Each model was trained for 1000 optimizer steps, sufficient to observe early-phase dynamics where most geometric effects. During training, the following quantities were logged:</p> <ul> <li>Gradient norms (to track spike frequency and stability),</li> <li>Training loss (for convergence smoothness),</li> <li>Cosine similarity between consecutive gradients (to measure local coherence), and</li> <li>The largest eigenvalue of the Hessian, $\lambda_{\text{max}}$​, estimated every 100 steps using the power-iteration method.</li> </ul> <p>The largest eigenvalue of the Hessian, $\lambda_{\text{max}}$, serves as a practical proxy for the curvature of the loss surface: large eigenvalues indicate sharper, more unstable regions where gradients fluctuate rapidly, while smaller values correspond to flatter, better-conditioned areas that yield smoother updates and greater training stability.</p> <p><br/></p> <style>.img-grid{display:grid;grid-template-columns:repeat(2,1fr);column-gap:12px;row-gap:24px}.img-grid img{width:100%;height:auto;display:block}</style> <div class="img-grid"> <img src="/assets/img/gradient_of_thought_figures/loss.png" alt=""/> <img src="/assets/img/gradient_of_thought_figures/norm.png" alt=""/> <img src="/assets/img/gradient_of_thought_figures/cos_sim.png" alt=""/> <img src="/assets/img/gradient_of_thought_figures/hessian.png" alt=""/> </div> <p><br/> Across all runs, the training loss decreased steadily for roughly the first 150 steps before settling into a shallow oscillatory regime. This pattern is typical for low learning-rate fine-tuning on an already instruction-trained model: most of the easy loss reduction happens early. Chain-of-Thought supervision produced smaller gradient norms on average and fewer sharp spikes, suggesting that its updates were more tempered than those of direct answer-only training. The cosine similarities between consecutive gradients were broadly similar across all runs, implying that the overall direction of descent was preserved; what changed was the scale and smoothness of the steps.</p> <p>The most revealing signal though came from the <strong>curvature estimates</strong>. The largest Hessian eigenvalue, $\lambda_{\max}$, was consistently lower under reasoning supervision—approximately two-thirds of the value observed in the direct, answer-only baseline. Because $\lambda_{\max}$ quantifies the steepest curvature of the loss surface, this reduction indicates that CoT training leads the model into a <strong>flatter and better-conditioned region</strong> of parameter space, something that learning rate cannot change. In such regions, small perturbations in the weights produce proportionally smaller changes in loss; optimization proceeds more predictably, and the same learning rate becomes effectively more stable. Lower curvature implies that nearby parameter configurations behave similarly: that reasoning supervision smooths not the gradients themselves, but the <strong>landscape they inhabit</strong>.</p> <p><br/></p> <h1> Reasoning as Implicit Alignment </h1> <p><br/></p> <p>For the most part, changes in logit gradients translate directly into changes in parameter gradients. The experiment above suggests that reasoning supervision affects more than the scale of these gradients: it changes the geometry of the loss surface itself. Models trained on reasoning traces converge into regions of lower curvature. This flattening implies that nearby parameter configurations yield similar loss values, so the model’s updates remain better-conditioned and less sensitive to noise. In geometric terms, reasoning supervision moves the optimizer into flatter, wider basins, where learning proceeds more stably and with smaller deviations from the original pre-trained parameters.</p> <p>This behavior supports the view that reasoning supervision functions more as elicitation than just memorization. By distributing surprisal across intermediate steps of thought, Chain-of-Thought supervision regularizes the geometry of the loss landscape, improving its curvature and conditioning. The optimizer moves through flatter, better-behaved regions of parameter space, allowing updates to remain stable and directionally consistent under the same training conditions. In this regime, new reasoning patterns can be integrated without distorting the knowledge that underlies them: the model learns to refine what it already knows rather than overwrite it. Combined with the smaller gradient updates, reasoning supervision in post-training promotes less memorization and catastrophic forgetting, and guides learning toward coherence, making the parameters stay closer to the pre-trained model while aligning its behavior more closely with human-like reasoning. Philosophically, this suggests that reasoning is not merely a tool for alignment, but a natural objective for it: a way of teaching the model to think by reshaping the very geometry through which it learns.</p> <p>In essence, pre-training yields a vast but rugged terrain, full of steep cliffs where token probabilities shift abruptly from one step to the next. Post-training on reasoning traces acts less like the pure addition of new knowledge, but a bit more like a smoothing operation across this surface. The same information remains, but its contours soften; gradients that once fluctuated violently begin to flow coherently along intermediate steps. The model does not merely learn new facts—it learns to traverse its own knowledge more gracefully. In the end, the gradient tells a simple story: reasoning is a way of making prediction coherent. When a model learns to smooth its own landscape—distributing complexity over time and thought—it not only mimics reasoning better, but also becomes a gentler learner.</p> <p><br/></p> <h1> References </h1> <p><br/></p> <p id="ref1">[1] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V. Le, and Denny Zhou. 2022. <i>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.</i> In Advances in Neural Information Processing Systems 35 (NeurIPS 2022).</p>]]></content><author><name></name></author><category term="ai"/><category term="nlp"/><summary type="html"><![CDATA[Large language models (LLMs) learn in a remarkably simple way: by predicting the next token in a sequence. During pre-training, they process vast corpora and adjust parameters $\theta$ so that the probability assigned to the correct next token,]]></summary></entry><entry><title type="html">Diophantine Approximation and Transcendental Numbers</title><link href="https://mikezhou.me/blog/2025/diophantine-approximation-and-transcendental-numbers/" rel="alternate" type="text/html" title="Diophantine Approximation and Transcendental Numbers"/><published>2025-04-12T22:48:00+00:00</published><updated>2025-04-12T22:48:00+00:00</updated><id>https://mikezhou.me/blog/2025/diophantine-approximation-and-transcendental-numbers</id><content type="html" xml:base="https://mikezhou.me/blog/2025/diophantine-approximation-and-transcendental-numbers/"><![CDATA[<p>Many numbers we encounter in everyday life–even those that seem impossibly complex–fall into categories we’ve known for centuries: rational numbers, which can be written in the form \(p/q\), and algebraic numbers, which can be written as the root of some rational polynomial. But beyond these well-behaved figures lies a stranger world: the realm of transcendental numbers.</p> <p>Transcendental numbers are numbers that cannot be expressed as solutions to polynomial equations with integer coefficients. Up until the mid-19th century, it was unresolved whether any transcendental numbers existed. In fact, it is extremely hard to even prove that a number may even be transcendental. While Liouville had proven the existence of transcendental numbers in 1844, it wasn’t until 1873 that Hermite proved \(e\) is transcendental, followed by Lindemann’s proof of \(\pi\)’s transcendence in 1882. Liouville’s approach, centered on approximation properties of numbers, provided the first concrete examples of transcendental numbers and established a powerful criterion for transcendence.</p> <p>To understand the intuition of Liouville’s approach, let’s first walk through Cantor’s proof of the existence of transcendental numbers. To understand why transcendental numbers must exist, we begin with a key concept from set theory: the idea of countable and uncountable sets.</p> <p>A set is said to be <b>countable</b> if its elements can be placed into a one-to-one correspondence with the natural numbers \(\mathbb{N} = \{1, 2, 3, \dots\}\). That is, we can enumerate the elements in a list: \(x_1, x_2, x_3, \dots\). Examples of countable sets include:</p> <ul> <li>The set of natural numbers \(\mathbb{N}\)</li> <li>The set of integers \(\mathbb{Z}\)</li> <li>The set of rational numbers \(\mathbb{Q}\)</li> </ul> <p>For example, \(\mathbb{Q}\) is in one one-to-one correspondence to \(\mathbb{N}\) by “counting” rationals in order by the sum of their numerator and denominator (ie. \(1 / 1\) goes first, \(1 / 2\) goes second, \(2 / 1\) goes third, \(1 / 3\) goes fourth, \(2 / 2\) goes fifth, \(3 / 1\) goes sixth, etc). Similar to the proof that \(\mathbb{Q}\) is countable, the set of all integer polynomials is also countable, meaning that the set of all algebraic numbers is countable.</p> <p>Since all algebraic numbers are countable, it would suffice to show that the reals are uncountable in order to demonstrate the existence of transcendental numbers. To do this, Cantor supposed for contradiction that all real numbers in the interval \((0, 1)\) can be listed:</p> \[r_1 = 0.a_{11}a_{12}a_{13}\dots\] \[r_2 = 0.a_{21}a_{22}a_{23}\dots\] \[r_3 = 0.a_{31}a_{32}a_{33}\dots\] \[\vdots\] <p>Now construct a new number \(r \in (0,1)\) by altering the diagonal digits:</p> \[b_i = \begin{cases} 1 &amp; \text{if } a_{ii} \neq 1 \\ 0 &amp; \text{if } a_{ii} = 1 \end{cases}\] <p>and define</p> \[r = 0.b_1 b_2 b_3 \dots\] <p>By construction, \(r\) differs from each \(r_i\) in at least the \(i\)-th decimal place. Therefore, \(r\) is not in the list, contradicting our assumption that all real numbers in \((0,1)\) were listed.</p> <p>From this result, we can make a clear observation. While the rational and algebraic numbers are scattered densely across the real line, they form only a countable skeleton. The transcendental numbers, in contrast, fill in the uncountable “gaps” between them, making up almost all of the real number continuum. In essence, because transcendental numbers fill in these gaps between rationals, they can reveal themselves through how well they can be approximated by rationals. Roughly speaking, an irrational number that can be approximated by rationals better than any other number must be transcendental.</p> <h3>Liouville's Insight and Transcendental Numbers</h3> <p>Though predating Cantor, Liouville’s crucial insight, dating back to 1844, was exactly this: to understand just how well irrational algebraic numbers can be approximated by rationals—and how that imposes deep structural limitations. To demonstrate this, he constructed what is now known as the <b>Liouville constant</b>:</p> \[L = \sum_{n=1}^{\infty} \frac{1}{10^{n!}} = 0.110001000000000000000001000\ldots\] <p>Now you might ask: what does it mean for a number to be “better” approximated by a rational? After all, every real number admits rational approximations. The usual intention is to compare the denominator of the approximating fraction with the size of the error. An approximation is unusually good if the number can be matched by a rational whose denominator is small relative to the error term.</p> <p>Let’s begin with the key result:</p> <p><strong>Theorem 1 (Liouville).</strong> <em>Let</em> \(\alpha\) <em>be an irrational algebraic number of degree</em> \(n &gt; 1\). <em>Then there exists a positive constant</em> \(A\) <em>such that for all integers</em> \(p, q\) <em>with</em> \(q &gt; 0\), we have:</p> \[\left\vert \alpha - \frac{p}{q}\right\vert &gt; \frac{A}{q^n}\] <p>In other words, irrational algebraic numbers can’t be too well approximated by rational numbers, and any number that can be approximated well by a rational of small denominator must be transcendental.</p> <p><em>Proof.</em> Let \(f(x) = \sum a_k x^k\) be the minimal polynomial of \(\alpha\) with integer coefficients. Since \(f(\alpha) = 0\), we’ll examine how \(f\) behaves near \(\alpha\).</p> <p>By the fundamental theorem of algebra, \(f\) has at most \(n\) roots. This means there exists some \(\delta_1 &gt; 0\) such that if \(0 &lt; \vert x - \alpha \vert &lt; \delta_1\), then \(f(x) \neq 0\).</p> <p>Also, since \(f'(\alpha) \neq 0\) and \(f'\) is continuous, we can find \(\delta_2 &gt; 0\) and a bound \(M &gt; 0\) such that if \(\vert x - \alpha \vert &lt; \delta_2\), then</p> \[0 &lt; |f'(x)| \leq M\] <p>Let \(\delta = \min(\delta_1, \delta_2)\). If a rational number \(\frac{p}{q}\) lies within \(\delta\) of \(\alpha\), then by the Mean Value Theorem, there exists \(x_0\) between \(p/q\) and \(\alpha\) such that</p> \[f'(x_0) = \frac{f(\alpha) - f(p/q)}{\alpha - p/q}\] <p>Since \(f(\alpha) = 0\) and \(f(p/q) \neq 0\), we rearrange:</p> \[\vert \alpha - p/q \vert = \frac{\vert f(p/q) \vert}{\vert f'(x_0) \vert}\] <p>Now consider the value of \(f(p/q)\). Since \(f\) has integer coefficients,</p> \[f(p/q) = \frac{1}{q^n} \sum_{k=0}^{n} a_k p^k q^{n-k}\] <p>This sum is an integer, and if \(f(p/q) \neq 0\), its absolute value is at least 1. So,</p> \[|\alpha - p/q| \geq \frac{1}{|f'(x_0)|} \cdot \frac{1}{q^n} \geq \frac{1}{M} \cdot \frac{1}{q^n}\] <p>Setting \(A = 1/M\), we get the desired bound.</p> <p>We then define a <strong>Liouville number</strong> as an irrational number \(x\) such that, for every integer \(n &gt; 0\), there exists a pair \((p, q)\) satisfying</p> \[\left\vert x - \frac{p}{q} \right\vert &lt; \frac{1}{q^n}\] <p>These numbers are, in a sense, <em>too well</em> approximated by rationals to be algebraic, and are better approximated by rationals than any other algebraic number.</p> <p><strong>Theorem 2.</strong> <em>Every Liouville number is transcendental.</em></p> <p><em>Proof.</em> Suppose not—suppose there’s a Liouville number \(x\) that is algebraic of degree \(d &gt; 1\). Then by Theorem 1, there is a constant \(A &gt; 0\) such that:</p> \[\left\vert x - \frac{p}{q} \right\vert &gt; \frac{A}{q^d}\] <p>for all rational numbers \(p/q\).</p> <p>But the definition of Liouville number tells us that for any \(n &gt; d\), we can find \(p/q\) such that</p> \[\left\vert x - \frac{p}{q} \right\vert &lt; \frac{1}{q^n}\] <p>For sufficiently large \(n\), we have the inequality</p> \[\frac{1}{q^n} &lt; \frac{A}{q^d}\] <p>contradicting the lower bound. Therefore, \(x\) cannot be algebraic.</p> <p>Returning to Liouville’s original construction:</p> \[L = \sum_{n=1}^{\infty} \frac{1}{10^{n!}}\] <p>We’ll show that this specific number satisfies the Liouville condition.</p> <p><strong>Theorem 3.</strong> <em>The Liouville constant</em> \(L\) <em>is transcendental.</em></p> <p><em>Proof.</em> For any \(m &gt; 0\), define a rational approximation:</p> \[L_m = \sum_{n=1}^{m} \frac{1}{10^{n!}}\] <p>This is a rational number with denominator \(10^{m!}\). The error between \(L\) and \(L_m\) is:</p> \[|L - L_m| = \sum_{n=m+1}^{\infty} \frac{1}{10^{n!}} = \frac{1}{9 \cdot 10^{(m+1)! - 1}}\] <p>Since \((m+1)! - 1 \geq m!(m)\) for large enough \(m\), we can write:</p> \[|L - L_m| &lt; \frac{1}{10^{m!(m)}} = \frac{1}{(10^{m!})^{m}} = \frac{1}{q^{m}}\] <p>where \(q = 10^{m!}\). Hence, by Theorem 2, \(L\) is transcendental.</p> <h3>Concluding Remarks: Roth’s Theorem on Diophantine Approximation</h3> <p>Liouville’s theorem was eventually sharpened in 1955 by Klaus Roth, who proved a striking improvement:</p> <p><strong>Theorem 4 (Roth).</strong> <em>Let</em> \(\alpha\) <em>be an irrational algebraic number. Then for every</em> \(\epsilon &gt; 0\)<em>, there exist only finitely many rational numbers \(p/q\) such that</em></p> \[\left\vert \alpha - \frac{p}{q} \right\vert &lt; \frac{1}{q^{2 + \epsilon}}\] <p>This result shows that the exponent \(2\) is essentially optimal for algebraic irrationals: they cannot be approximated better than order \(1/q^2\), no matter how high their degree.</p> <p>This study of how well real numbers can be approximated by rational ones is known as Diophantine Approximation. At first glance, this seems like a simple analytical question—how close can a fraction get to a given real number? But as it turns out, the answer is tightly constrained by number-theoretic structure. It forms in essence a conceptual bridge between analysis and number theory, revealing how the seemingly soft idea of “closeness” is secretly governed by hard algebraic facts. For instance, while any real number can be approximated by rationals, not all can be approximated equally well—and that difference can be telling.</p> <p>Transcendental numbers—those that satisfy no algebraic equation with integer coefficients—can often be detected by their approximation behavior. Liouville’s insight was not just a clever construction—it was the beginning of a powerful philosophy: that transcendence can be sensed in how a number weaves through the gaps left by the rationals. Some gaps are too intricate for algebra to capture.</p>]]></content><author><name></name></author><category term="math"/><category term="number-theory"/><summary type="html"><![CDATA[Many numbers we encounter in everyday life–even those that seem impossibly complex–fall into categories we’ve known for centuries: rational numbers, which can be written in the form \(p/q\), and algebraic numbers, which can be written as the root of some rational polynomial. But beyond these well-behaved figures lies a stranger world: the realm of transcendental numbers.]]></summary></entry><entry><title type="html">Hardy-Ramanujan with probabilistic method</title><link href="https://mikezhou.me/blog/2024/distinct-prime-factors-problem/" rel="alternate" type="text/html" title="Hardy-Ramanujan with probabilistic method"/><published>2024-10-21T13:57:00+00:00</published><updated>2024-10-21T13:57:00+00:00</updated><id>https://mikezhou.me/blog/2024/distinct-prime-factors-problem</id><content type="html" xml:base="https://mikezhou.me/blog/2024/distinct-prime-factors-problem/"><![CDATA[<p>One of my favourite examples of the probabilistic method at work is the Hardy–Ramanujan theorem. It says that if you take a large integer \(x\), the number of distinct prime factors it has, written \(\omega(x)\), is usually very close to \(\log\log x\). More precisely, the fluctuations are typically no larger than about \(\sqrt{\log\log x}\).</p> <p>At first sight this is the kind of statement one might expect to need advanced analytic tools. After all, it concerns the distribution of prime factors, which is usually a delicate subject. But in fact the proof rests on a very simple probabilistic observation; to sketch the main one out, if a random variable’s expectation and variance are “close enough”, one can use Chebyshev’s inequality to show that almost all cases of this random variable are assymptotically equal to its expectation with only a vanishing number of exceptions. Thus, if the expected value of \(\omega(x)\) is close to its variance, we can use clever bounds to find an elementary proof to a seemingly difficult number theoretic problem.</p> <p><b>Theorem (Ramanujan).</b> Let \(\phi\) be any function that grows arbitrarily slowly to infinity. For large enough \(N\), we have that \(\vert \{x \leq N :\vert \omega(x) - \text{log}(\text{log}(x)) \vert &gt; \phi(x)\sqrt{\text{log}(\text{log}(x))} \} \vert = o(N)\)</p> <p>To prove this statement, it suffices to show that \(\textbf{P}( \vert \omega(x) - \text{log}(\text{log}(x)) \vert &gt; \phi(x)\sqrt{\text{log}(\text{log}(x) )}) = o(1)\). This looks like an application of Chebyshev’s inequality.</p> <p>We can start by defining a random variable \(X\) denoting the number of prime divisors a randomly selected \(x \leq N\) has that are each less than \(N^{\frac{1}{3}}\) (notice that \(x\) can only have at most 3 prime factors larger than \(N^{\frac{1}{3}}\), so \(X\) will only be off by a constant additive factor). For a technical detail we will see later, it will be easier to analyze prime factors less than \(N^{\frac{1}{3}}\) rather than \(N\).</p> <p>Since the probability that a prime \(p\) divides \(x\) is \(\frac{1}{p} + O\left(\frac{1}{N}\right)\), we have that</p> \[\textbf{E}[X] = \sum_{p \leq N^{\frac{1}{3}}} \frac{1}{p} + O\left(\frac{1}{N}\right)\] <p>by simply letting \(X\) be the sum of indicator variables representing whether a prime divides \(x\) or not. Since each indicator variable is a Bernoulli random variable, their individual variances are \(\textbf{P}(p \mid x) - \textbf{P}(p \mid x)^2\), meaning that</p> \[\textbf{Var}(X) = \left(\sum_{p \leq N^{\frac{1}{3}}} \frac{1}{p} - \frac{1}{p^2} + O\left( \frac{1}{N} \right) \right) - 2\left(\sum_{p &lt; q \leq N^{\frac{1}{3}}} \textbf{E}[(\mathbb{I}(p \mid x))(\mathbb{I}(q \mid x))] - \textbf{E}[(\mathbb{I}(p \mid x))]\textbf{E}[(\mathbb{I}(q \mid x))] \right).\] <p>Notice that \((\mathbb{I}(p \mid x))(\mathbb{I}(q \mid x)) = 1\) if and only if both \(p\) and \(q\) divide \(x\). This is the same as if \(pq\) divides \(x\), which has probability \(\frac{1}{pq} + O\left(\frac{1}{N}\right)\). Thus, <br/> <br/></p> \[\textbf{E}[(\mathbb{I}(p \mid x))(\mathbb{I}(q \mid x))] - \textbf{E}[(\mathbb{I}(p \mid x))]\textbf{E}[(\mathbb{I}(q \mid x))]\] <p><br/></p> \[= \frac{1}{pq} + O\left(\frac{1}{N}\right) - \left(\frac{1}{p} + O\left(\frac{1}{N}\right)\right) \left(\frac{1}{q} + O\left(\frac{1}{N}\right) \right) = O\left(\frac{1}{N}\right)\] <p>and</p> \[\textbf{Var}(X) = \left(\sum_{p \leq N^{\frac{1}{3}}} \frac{1}{p} - \frac{1}{p^2} + O\left( \frac{1}{N}\right) \right) + 2\left(\sum_{p &lt; q \leq N^{\frac{1}{3}}} O\left( \frac{1}{N}\right)\right).\] <p>Since we are summing at most \(N^{\frac{2}{3}}\) number of terms (the technical detail we discussed earlier), we have that each \(O(\frac{1}{N})\) vanishes. We can then use Merten’s estimates and the fact that the sum of reciprical of squares converges to get that</p> \[\textbf{E}[X] = \text{log}(\text{log}(N)) + O(1)\] \[\textbf{Var}(X) = \text{log}(\text{log}(N)) + O(1)\] <p>and then by Chebyshev’s inequality and the fact that \(\text{log}(\text{log}(x)) = \text{log}(\text{log}(N)) + O(1)\) with probability \(1 - o(1)\), we have that</p> \[\textbf{P}\left( \vert \omega(x) - \text{log}(\text{log}(x)) \vert &gt; \phi(x)\sqrt{\text{log}(\text{log}(x) )}\right) \ll o(1) + \frac{\text{log}(\text{log}(N))}{\phi(x)^2\text{log}(\text{log}(N))} = o(1)\] <p>meaning that</p> \[\vert \{x \leq N :\vert \omega(x) - \text{log}(\text{log}(x)) \vert &gt; \phi(x)\sqrt{\text{log}(\text{log}(x))} \} \vert = o(N).\] <p><br/></p> <h3> Note on Merten Estimates </h3> <p>Merten’s estimates can be derived using elementary techniques. All you really need is that \(\text{log}(n!) = n\text{log}(n) + O(n)\), as</p> \[\text{log}(n!) = \sum_{p \leq n} \text{log}(p) \lfloor \frac{n}{p} \rfloor = O(n) + n\sum_{p \leq n} \frac{\text{log}(p)}{p}\] \[\sum_{p \leq n}\frac{\text{log}(p)}{p} = \text{log}(n) + O(1)\] <p>Then, using summation by parts on \(\frac{\text{log}(p)}{p}\) and \(\frac{1}{\text{log}(p)}\), we have that</p> \[\sum_{p \leq n} \frac{1}{p} = \text{log}(\text{log}(n)) + O(1)\]]]></content><author><name></name></author><category term="math"/><category term="combinatorics"/><category term="number-theory"/><summary type="html"><![CDATA[One of my favourite examples of the probabilistic method at work is the Hardy–Ramanujan theorem. It says that if you take a large integer \(x\), the number of distinct prime factors it has, written \(\omega(x)\), is usually very close to \(\log\log x\). More precisely, the fluctuations are typically no larger than about \(\sqrt{\log\log x}\).]]></summary></entry><entry><title type="html">Minimum number of lines from non-collinear points</title><link href="https://mikezhou.me/blog/2024/non-collinear-points-problem/" rel="alternate" type="text/html" title="Minimum number of lines from non-collinear points"/><published>2024-09-16T16:42:00+00:00</published><updated>2024-09-16T16:42:00+00:00</updated><id>https://mikezhou.me/blog/2024/non-collinear-points-problem</id><content type="html" xml:base="https://mikezhou.me/blog/2024/non-collinear-points-problem/"><![CDATA[<p>As I’ve been reading more about June Huh’s work with the Dowling-Wilson conjecture, I’ve become more and more interested in algebraic methods within combinatorics. More recently, I’ve come across a very cute problem with a very neat solution, inspired by simple ideas from linear algebra.</p> <p><b>Problem 1.</b> What is the minimum number of lines \(n\) non-collinear points form if all lines are drawn between points?</p> <p>It is easy to see that we can achieve \(n\) lines by simply leaving \(n - 1\) collinear points on a line and the last point off the line. Indeed, this is the best case possible.</p> <p>To demonstrate why, we can assign variables \(x\) to each of the points, and define a system of equations as:</p> \[\sum_{j: \text{ } x_j \in l_i} x_j = 0.\] <p>That is, the sum of all the variables on line \(i\) is equal to 0. Assuming that there is a placement of \(n\) points that produces less than \(n\) lines, we will have at most \(n - 1\) equations and \(n\) variables, meaning that there exists a nontrivial solution (this is the key idea of the proof). Since the square of non-zero numbers is strictly larger than 0, it would make sense to introduce squares, so let’s square each equation and add them all up, like this:</p> \[\sum_{i = 1}^{m}\left(\sum_{j:\text{ } x_j \in l_i} x_j\right)^2 = \sum_{i = 1}^n a_i x_i^2 + 2\sum_{1 \leq i &lt; j \leq n} a_{i, j} x_i x_j = 0\] <p>where \(m\) is the number of lines, \(a_i\) is the number of lines that \(x_i\) shows up on, and \(a_{i, j}\) is the number of lines \(x_i\) and \(x_j\) show up on. Since our points are not all on a single line, \(a_i &gt; 1\), and since all lines are drawn between points and two points define a line, we have that \(a_{i, j}\) equals exactly 1. Thus,</p> \[0 = \sum_{i = 1}^n a_i x_i^2 + 2\sum_{1 \leq i &lt; j \leq n} a_{i, j} x_i x_j = \sum_{i = 1}^n (a_i - 1) x_i^2 + \left(\sum_{i = 1}^n x_i \right)^2.\] <p>Because we have a non-trivial solution and \(a_i &gt; 1\),</p> \[\sum_{i = 1}^n (a_i - 1) x_i^2 &gt; 0,\] <p>and</p> \[0 = \sum_{i = 1}^n (a_i - 1) x_i^2 + \left(\sum_{i = 1}^n x_i \right)^2 &gt; 0\] <p>gives a contradiction.</p>]]></content><author><name></name></author><category term="math"/><category term="combinatorics"/><summary type="html"><![CDATA[As I’ve been reading more about June Huh’s work with the Dowling-Wilson conjecture, I’ve become more and more interested in algebraic methods within combinatorics. More recently, I’ve come across a very cute problem with a very neat solution, inspired by simple ideas from linear algebra.]]></summary></entry></feed>