<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://mzhou02.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mzhou02.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-31T21:53:57+00:00</updated><id>https://mzhou02.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Hardy-Ramanujan with probablistic method</title><link href="https://mzhou02.github.io/blog/2024/distinct-prime-factors-problem/" rel="alternate" type="text/html" title="Hardy-Ramanujan with probablistic method"/><published>2024-10-21T13:57:00+00:00</published><updated>2024-10-21T13:57:00+00:00</updated><id>https://mzhou02.github.io/blog/2024/distinct-prime-factors-problem</id><content type="html" xml:base="https://mzhou02.github.io/blog/2024/distinct-prime-factors-problem/"><![CDATA[<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-0823RLC0T3"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0823RLC0T3");</script> <p>One of my favorite applications of the probablistic method is perhaps the Hardy-Ramanujan theorem in number theory, which roughly states that almost all numbers \(x\) have about less than \(\text{log}(\text{log}(x))\) number of prime divisors, \(\omega(x)\).</p> <p><b>Theorem (Ramanujan).</b> Let \(\phi\) be any function that grows arbitrarily slowly to infinity. For large enough \(N\), we have that \(\vert \{x \leq N :\vert \omega(x) - \text{log}(\text{log}(x)) \vert &gt; \phi(x)\sqrt{\text{log}(\text{log}(x))} \} \vert = o(N)\)</p> <p>To prove this statement, it suffices to show that \(\textbf{P}( \vert \omega(x) - \text{log}(\text{log}(x)) \vert &gt; \phi(x)\sqrt{\text{log}(\text{log}(x) )}) = o(1)\). This looks like an application of Chebyshev’s inequality.</p> <p>To begin, let \(X\) be the random variable equal denoting the number of prime divisors a randomly selected \(x \leq N\) has that are each less than \(N^{\frac{1}{3}}\) (notice that \(x\) can only have at most 3 prime factors larger than \(N^{\frac{1}{3}}\), so \(X\) will only be off by a constant additive factor). Since the probability that a prime \(p\) divides \(x\) is \(\frac{1}{p} + O\left(\frac{1}{N}\right)\), we have that</p> \[\textbf{E}[X] = \sum_{p \leq N^{\frac{1}{3}}} \frac{1}{p} + O\left(\frac{1}{N}\right)\] <p>by simply letting \(X\) be the sum of indicator variables representing whether a prime divides \(x\) or not. Since each indicator variable is a Bernoulli random variable, their individual variances are \(\frac{1}{p} - \frac{1}{p^2}\), meaning that</p> \[\textbf{Var}(X) = \left(\sum_{p \leq N^{\frac{1}{3}}} \frac{1}{p} - \frac{1}{p^2} \right) - 2\left(\sum_{p &lt; q \leq N^{\frac{1}{3}}} \textbf{E}[(\mathbb{I}(p \mid x))(\mathbb{I}(q \mid x))] - \textbf{E}[(\mathbb{I}(p \mid x))]\textbf{E}[(\mathbb{I}(q \mid x))] \right)\] <p>Notice that \((\mathbb{I}(p \mid x))(\mathbb{I}(q \mid x)) = 1\) if and only if both \(p\) and \(q\) divide \(x\). This is the same as if \(pq\) divides \(x\), which has probability \(\frac{1}{pq} + O\left(\frac{1}{N}\right)\). Thus,</p> \[\textbf{E}[(\mathbb{I}(p \mid x))(\mathbb{I}(q \mid x))] - \textbf{E}[(\mathbb{I}(p \mid x))]\textbf{E}[(\mathbb{I}(q \mid x))]\] \[= \frac{1}{pq} + O\left(\frac{1}{N}\right) - \left(\frac{1}{p} + O\left(\frac{1}{N}\right)\right) \left(\frac{1}{q} + O\left(\frac{1}{N}\right) \right) = O\left(\frac{1}{N}\right)\] <p>Thus,</p> \[\textbf{Var}(X) = \left(\sum_{p \leq N^{\frac{1}{3}}} \frac{1}{p} - \frac{1}{p^2} \right) + 2\left(\sum_{p &lt; q \leq N^{\frac{1}{3}}} O\left( \frac{1}{N}\right)\right)\] <p>Using Merten estimates and the fact that the sum of reciprical of squares converges, we get that</p> \[\textbf{E}[X] = \text{log}(\text{log}(N)) + O(1)\] \[\textbf{Var}(X) = \text{log}(\text{log}(N)) + O(1)\] <p>and then by Chebyshev’s inequality and the fact that \(\text{log}(\text{log}(x)) = \text{log}(\text{log}(x)) + O(1)\) with probability \(1 - o(1)\), we have that</p> \[\textbf{P}\left( \vert \omega(x) - \text{log}(\text{log}(x)) \vert &gt; \phi(x)\sqrt{\text{log}(\text{log}(x) )}\right) \ll o(1) + \frac{\text{log}(\text{log}(N))}{\phi(x)^2\text{log}(\text{log}(N))} = o(1)\] <p>and we are done.</p>]]></content><author><name></name></author><category term="math"/><category term="combinatorics,"/><category term="number-theory"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Minimum number of lines from non-collinear points</title><link href="https://mzhou02.github.io/blog/2024/non-collinear-points-problem/" rel="alternate" type="text/html" title="Minimum number of lines from non-collinear points"/><published>2024-09-16T16:42:00+00:00</published><updated>2024-09-16T16:42:00+00:00</updated><id>https://mzhou02.github.io/blog/2024/non-collinear-points-problem</id><content type="html" xml:base="https://mzhou02.github.io/blog/2024/non-collinear-points-problem/"><![CDATA[<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-0823RLC0T3"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0823RLC0T3");</script> <p>Recently, I’ve been reading more about algebraic methods within combinatorics, and have been fascinated by the appearance of topics like linear algebra in the subject. In this blog post, I’ll share one of my favorite problems I’ve come across lately that is really just inspired by some incredibly simple ideas.</p> <p><b>Problem 1.</b> What is the minimum number of lines \(n\) non-collinear points form if all lines are drawn between points?</p> <p>It is easy to see that we can achieve \(n\) lines by simply leaving \(n - 1\) collinear points on a line and the last point off the line. Indeed, this is the best case possible.</p> <p>To demonstrate why, we can assign variables \(x\) to each of the points, and define a system of equations as:</p> \[\sum_{j: \text{ } x_j \in l_i} x_j = 0.\] <p>That is, the sum of all the variables on line \(i\) is equal to 0. Assuming that there is a placement of \(n\) points that produces less than \(n\) lines, we will have at most \(n - 1\) equations and \(n\) variables, meaning that there exists a nontrivial solution (this is the key idea of the proof). Since the square of non-zero numbers is strictly larger than 0, it would make sense to introduce squares, so let’s square each equation and add them all up, like this:</p> \[\sum_{i = 1}^{m}\left(\sum_{j:\text{ } x_j \in l_i} x_j\right)^2 = \sum_{i = 1}^n a_i x_i^2 + 2\sum_{1 \leq i &lt; j \leq n} a_{i, j} x_i x_j = 0\] <p>where \(m\) is the number of lines, \(a_i\) is the number of lines that \(x_i\) shows up on, and \(a_{i, j}\) is the number of lines \(x_i\) and \(x_j\) show up on. Since our points are not all on a single line, \(a_i &gt; 1\), and since all lines are drawn between points and two points define a line, we have that \(a_{i, j}\) equals exactly 1. Thus,</p> \[0 = \sum_{i = 1}^n a_i x_i^2 + 2\sum_{1 \leq i &lt; j \leq n} a_{i, j} x_i x_j = \sum_{i = 1}^n (a_i - 1) x_i^2 + \left(\sum_{i = 1}^n x_i \right)^2.\] <p>Because we have a non-trivial solution and \(a_i &gt; 1\),</p> \[\sum_{i = 1}^n (a_i - 1) x_i^2 &gt; 0,\] <p>and</p> \[0 = \sum_{i = 1}^n (a_i - 1) x_i^2 + \left(\sum_{i = 1}^n x_i \right)^2 &gt; 0\] <p>gives a contradiction.</p>]]></content><author><name></name></author><category term="math"/><category term="combinatorics"/><summary type="html"><![CDATA[]]></summary></entry></feed>