<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://mzhou02.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mzhou02.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-16T21:16:29+00:00</updated><id>https://mzhou02.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Hardy-Ramanujan with probabilistic method</title><link href="https://mzhou02.github.io/blog/2024/distinct-prime-factors-problem/" rel="alternate" type="text/html" title="Hardy-Ramanujan with probabilistic method"/><published>2024-10-21T13:57:00+00:00</published><updated>2024-10-21T13:57:00+00:00</updated><id>https://mzhou02.github.io/blog/2024/distinct-prime-factors-problem</id><content type="html" xml:base="https://mzhou02.github.io/blog/2024/distinct-prime-factors-problem/"><![CDATA[<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-0823RLC0T3"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0823RLC0T3");</script> <p>One of my favorite applications of the probabilistic method is perhaps the Hardy-Ramanujan theorem in number theory, which <i>roughly</i> states that almost all numbers \(x\) have approximately \(\text{log}(\text{log}(x))\) number of prime divisors. In more precise terms, the number of prime divisors \(x\) has, denoted as denoted as \(\omega(x)\), tends to \(\text{log}(\text{log}(x))\), with only an error term that approaches \(O\left( \sqrt{\text{log}(\text{log}(x))} \right)\) (ie. raise it to a \(1 + \epsilon\) power), for nearly all \(x\). This statement might seem a bit difficult to prove, as an intuitive approach would most likely start with advanced tools like the prime number theorem. But in reality, it really just involves some simple ideas; to sketch the main one out, if a random variable’s expectation and variance are “close enough”, one can use Chebyshev’s inequality to show that almost all cases of this random variable are assymptotically equal to its expectation with only a vanishing number of exceptions. Thus, if the expected value of \(\omega(x)\) is close to its variance, we can use clever bounds to find an elementary proof to a seemingly difficult number theoretic problem.</p> <p><b>Theorem (Ramanujan).</b> Let \(\phi\) be any function that grows arbitrarily slowly to infinity. For large enough \(N\), we have that \(\vert \{x \leq N :\vert \omega(x) - \text{log}(\text{log}(x)) \vert &gt; \phi(x)\sqrt{\text{log}(\text{log}(x))} \} \vert = o(N)\)</p> <p>To prove this statement, it suffices to show that \(\textbf{P}( \vert \omega(x) - \text{log}(\text{log}(x)) \vert &gt; \phi(x)\sqrt{\text{log}(\text{log}(x) )}) = o(1)\). This looks like an application of Chebyshev’s inequality.</p> <p>We can start by defining a random variable \(X\) denoting the number of prime divisors a randomly selected \(x \leq N\) has that are each less than \(N^{\frac{1}{3}}\) (notice that \(x\) can only have at most 3 prime factors larger than \(N^{\frac{1}{3}}\), so \(X\) will only be off by a constant additive factor). For a technical detail we will see later, it will be easier to analyze prime factors less than \(N^{\frac{1}{3}}\) rather than \(N\).</p> <p>Since the probability that a prime \(p\) divides \(x\) is \(\frac{1}{p} + O\left(\frac{1}{N}\right)\), we have that</p> \[\textbf{E}[X] = \sum_{p \leq N^{\frac{1}{3}}} \frac{1}{p} + O\left(\frac{1}{N}\right)\] <p>by simply letting \(X\) be the sum of indicator variables representing whether a prime divides \(x\) or not. Since each indicator variable is a Bernoulli random variable, their individual variances are \(\textbf{P}(p \mid x) - \textbf{P}(p \mid x)^2\), meaning that</p> \[\textbf{Var}(X) = \left(\sum_{p \leq N^{\frac{1}{3}}} \frac{1}{p} - \frac{1}{p^2} + O\left( \frac{1}{N} \right) \right) - 2\left(\sum_{p &lt; q \leq N^{\frac{1}{3}}} \textbf{E}[(\mathbb{I}(p \mid x))(\mathbb{I}(q \mid x))] - \textbf{E}[(\mathbb{I}(p \mid x))]\textbf{E}[(\mathbb{I}(q \mid x))] \right).\] <p>Notice that \((\mathbb{I}(p \mid x))(\mathbb{I}(q \mid x)) = 1\) if and only if both \(p\) and \(q\) divide \(x\). This is the same as if \(pq\) divides \(x\), which has probability \(\frac{1}{pq} + O\left(\frac{1}{N}\right)\). Thus, <br/> <br/></p> \[\textbf{E}[(\mathbb{I}(p \mid x))(\mathbb{I}(q \mid x))] - \textbf{E}[(\mathbb{I}(p \mid x))]\textbf{E}[(\mathbb{I}(q \mid x))]\] <p><br/></p> \[= \frac{1}{pq} + O\left(\frac{1}{N}\right) - \left(\frac{1}{p} + O\left(\frac{1}{N}\right)\right) \left(\frac{1}{q} + O\left(\frac{1}{N}\right) \right) = O\left(\frac{1}{N}\right)\] <p>and</p> \[\textbf{Var}(X) = \left(\sum_{p \leq N^{\frac{1}{3}}} \frac{1}{p} - \frac{1}{p^2} + O\left( \frac{1}{N}\right) \right) + 2\left(\sum_{p &lt; q \leq N^{\frac{1}{3}}} O\left( \frac{1}{N}\right)\right).\] <p>Since we are summing at most \(N^{\frac{2}{3}}\) number of terms (the technical detail we discussed earlier), we have that each \(O(\frac{1}{N})\) vanishes. We can then use Merten’s estimates and the fact that the sum of reciprical of squares converges to get that</p> \[\textbf{E}[X] = \text{log}(\text{log}(N)) + O(1)\] \[\textbf{Var}(X) = \text{log}(\text{log}(N)) + O(1)\] <p>and then by Chebyshev’s inequality and the fact that \(\text{log}(\text{log}(x)) = \text{log}(\text{log}(N)) + O(1)\) with probability \(1 - o(1)\), we have that</p> \[\textbf{P}\left( \vert \omega(x) - \text{log}(\text{log}(x)) \vert &gt; \phi(x)\sqrt{\text{log}(\text{log}(x) )}\right) \ll o(1) + \frac{\text{log}(\text{log}(N))}{\phi(x)^2\text{log}(\text{log}(N))} = o(1)\] <p>meaning that</p> \[\vert \{x \leq N :\vert \omega(x) - \text{log}(\text{log}(x)) \vert &gt; \phi(x)\sqrt{\text{log}(\text{log}(x))} \} \vert = o(N).\] <p><br/></p> <h3> Note on Merten Estimates </h3> <p>Merten’s estimates can be derived using elementary techniques. All you really need is that \(\text{log}(n!) = n\text{log}(n) + O(n)\), as</p> \[\text{log}(n!) = \sum_{p \leq n} \text{log}(p) \lfloor \frac{n}{p} \rfloor = O(n) + n\sum_{p \leq n} \frac{\text{log}(p)}{p}\] \[\sum_{p \leq n}\frac{\text{log}(p)}{p} = \text{log}(n) + O(1)\] <p>Then, using summation by parts on \(\frac{\text{log}(p)}{p}\) and \(\frac{1}{\text{log}(p)}\), we have that</p> \[\sum_{p \leq n} \frac{1}{p} = \text{log}(\text{log}(n)) + O(1)\]]]></content><author><name></name></author><category term="math"/><category term="combinatorics"/><category term="number-theory"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Minimum number of lines from non-collinear points</title><link href="https://mzhou02.github.io/blog/2024/non-collinear-points-problem/" rel="alternate" type="text/html" title="Minimum number of lines from non-collinear points"/><published>2024-09-16T16:42:00+00:00</published><updated>2024-09-16T16:42:00+00:00</updated><id>https://mzhou02.github.io/blog/2024/non-collinear-points-problem</id><content type="html" xml:base="https://mzhou02.github.io/blog/2024/non-collinear-points-problem/"><![CDATA[<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-0823RLC0T3"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0823RLC0T3");</script> <p>Recently, I’ve been reading more about algebraic methods within combinatorics, and have been fascinated by the appearance of topics like linear algebra in the subject. In this blog post, I’ll share one of my favorite problems I’ve come across lately that is really just inspired by some incredibly simple ideas.</p> <p><b>Problem 1.</b> What is the minimum number of lines \(n\) non-collinear points form if all lines are drawn between points?</p> <p>It is easy to see that we can achieve \(n\) lines by simply leaving \(n - 1\) collinear points on a line and the last point off the line. Indeed, this is the best case possible.</p> <p>To demonstrate why, we can assign variables \(x\) to each of the points, and define a system of equations as:</p> \[\sum_{j: \text{ } x_j \in l_i} x_j = 0.\] <p>That is, the sum of all the variables on line \(i\) is equal to 0. Assuming that there is a placement of \(n\) points that produces less than \(n\) lines, we will have at most \(n - 1\) equations and \(n\) variables, meaning that there exists a nontrivial solution (this is the key idea of the proof). Since the square of non-zero numbers is strictly larger than 0, it would make sense to introduce squares, so let’s square each equation and add them all up, like this:</p> \[\sum_{i = 1}^{m}\left(\sum_{j:\text{ } x_j \in l_i} x_j\right)^2 = \sum_{i = 1}^n a_i x_i^2 + 2\sum_{1 \leq i &lt; j \leq n} a_{i, j} x_i x_j = 0\] <p>where \(m\) is the number of lines, \(a_i\) is the number of lines that \(x_i\) shows up on, and \(a_{i, j}\) is the number of lines \(x_i\) and \(x_j\) show up on. Since our points are not all on a single line, \(a_i &gt; 1\), and since all lines are drawn between points and two points define a line, we have that \(a_{i, j}\) equals exactly 1. Thus,</p> \[0 = \sum_{i = 1}^n a_i x_i^2 + 2\sum_{1 \leq i &lt; j \leq n} a_{i, j} x_i x_j = \sum_{i = 1}^n (a_i - 1) x_i^2 + \left(\sum_{i = 1}^n x_i \right)^2.\] <p>Because we have a non-trivial solution and \(a_i &gt; 1\),</p> \[\sum_{i = 1}^n (a_i - 1) x_i^2 &gt; 0,\] <p>and</p> \[0 = \sum_{i = 1}^n (a_i - 1) x_i^2 + \left(\sum_{i = 1}^n x_i \right)^2 &gt; 0\] <p>gives a contradiction.</p>]]></content><author><name></name></author><category term="math"/><category term="combinatorics"/><summary type="html"><![CDATA[]]></summary></entry></feed>